### use # to comment out the configure item

### I/O ###
train_dir=sample_data/train.cappos.mtl
dev_dir=sample_data/dev.cappos.mtl
test_dir=sample_data/dev.cappos.mtl
model_dir=/tmp/cappos.mtl
word_emb_dir=sample_data/sample.word.emb

#raw_dir=
#decode_dir=
#dset_dir=
#load_model_dir=
#char_emb_dir=

norm_word_emb=False
norm_char_emb=False
number_normalized=True
seg=True
word_emb_dim=50
char_emb_dim=30

###NetworkConfiguration###
use_crf=True
use_char=True
word_seq_feature=LSTM
char_seq_feature=CNN
#feature=[POS] emb_size=20
#feature=[Cap] emb_size=20
#nbest=1

###TrainingSetting###
status=train
optimizer=SGD
iteration=10
batch_size=10
ave_batch_loss=False

###Hyperparameters###
cnn_layer=4
char_hidden_dim=50
hidden_dim=200
dropout=0.5
lstm_layer=1
bilstm=True
learning_rate=0.015
lr_decay=0.05
momentum=0
l2=1e-8
#gpu
#clip=

main_tasks=2
tasks=2
tasks_weights=1|1
#optimize_with_evalb Set to true only if you are doing constituent parsing 
#and wish to do model selection using the script evalb 
optimize_with_evalb=False


###Path to additional scripts specific for constituent parsing and the paper "Better, Faster, Stronger Sequence Tagging Constituent Parsers###
##tree2labels can be obtained from https://github.com/aghie/tree2labels
#tree2labels=tree2labels
#en2mt=tree2labels/encoding2multitask.py
#evaluate=tree2labels/evaluate.py
##evalb be set to the evalb script for PTB or it's SPMRL version if analyzing SPMRL datasets
#evalb=tree2labels/EVAL_SPRML/evalb_spmrl2013.final/evalb_spmrl
#gold_dev_trees=sample_data/cp_datasets/HEBREW_pred_tags/dev.Hebrew.pred.ptb

##gold_train_trees is only needed if fine-tuning with policy gradient and reinforcement learning 
#gold_train_trees=sample_data/cp_datasets/HEBREW_pred_tags/train5k.Hebrew.pred.ptb

